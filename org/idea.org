#+TITLE: Adaptive shrinkage via unconstrained optimization
#+SETUPFILE: setup.org

* Introduction

  ~ash~ relies on optimization of the following objective:

  \[ \pi^* = \arg\max_\pi \sum_j \ln \sum_k \pi_k p(x \mid \sigma^2_k, \cdot) \]

  where \(\pi\) is constrained to the probability simplex.

  [[https://link.springer.com/article/10.1023/A:1007558615313][Mackay 1998]] suggests that a change of basis to the /softmax basis/ may make
  the problem easier:

  \[ \pi_j = \frac{\exp(\alpha_j)}{\sum_k \exp(\alpha_k)} \]

  where \(\alpha_j\) are unconstrained reals.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="4G", venv="nwas") :dir /scratch/midway2/aksarkar/ash

  #+RESULTS:
  : Submitted batch job 43407569

  #+BEGIN_SRC ipython
    import numpy as np
    import scipy.optimize as so
    import scipy.stats as st
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[28]:
  :END:

* Implementation

  For simplicity, assume:

  1. The likelihood is Gaussian
  2. The prior is a scale mixture of Gaussians, with the default grid of scales
     as in [[https://academic.oup.com/biostatistics/article/18/2/275/2557030][Stephens 2016]].

  #+BEGIN_SRC ipython
    def softmax(logits):
      shifted = np.exp(logits - logits.max())
      return shifted / shifted.sum()

    def loss(logits, llik):
      return (llik * softmax(logits)).mean()

    def ash(beta, se, grid=None):
      if grid is None:
        start = se.min() / 10
        m = max(beta - se)
        if m < 0:
          end = 8 * start
        else:
          end = 2 * np.sqrt(m)
        grid = np.logspace(start, end, base=np.sqrt(2))
      assert len(grid.shape) == 1
      assert (grid > 0).all()
      var = se.reshape(-1, 1) + grid.reshape(1, -1)
      llik = -.5 * (np.log(2 * np.pi * var) + np.square(beta).reshape(-1, 1) / var)
      # This is tensorflow terminology (?)
      logits = np.zeros(grid.shape)
      res = so.minimize(loss, logits, args=(llik,))
      if res.success:
        return softmax(res.x)
      else:
        raise ValueError(res.message)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[48]:
  :END:

  Simulate some data, fixing \(\hat{s}_j = 1\) for all \(j\).

  #+BEGIN_SRC ipython
    def simulate(n, logits=None, grid=None):
      if grid is None:
        grid = np.geomspace(.1, 1, 5)
      assert len(grid.shape) == 1
      assert (grid > 0).all()
      if logits is None:
        logits = np.ones(grid.shape)
      assert logits.shape == grid.shape
      return np.random.normal(scale=np.random.choice(grid, p=softmax(logits), size=n))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[15]:
  :END:

  #+BEGIN_SRC ipython
    n = 100
    betahat = simulate(n=n)
    res = ash(betahat, np.ones(n))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[49]:
  :END:
