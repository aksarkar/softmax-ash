#+TITLE: Adaptive shrinkage via unconstrained optimization
#+SETUPFILE: setup.org

* Introduction

  ~ash~ relies on optimization of the following objective:

  \[ \pi^* = \arg\max_\pi \sum_j \ln \sum_k \pi_k p(x \mid \sigma^2_k, \cdot) \]

  where \(\pi\) is constrained to the probability simplex.

  [[https://link.springer.com/article/10.1023/A:1007558615313][Mackay 1998]] suggests that a change of basis to the /softmax basis/ may make
  the problem easier:

  \[ \pi_j = \frac{\exp(\alpha_j)}{\sum_k \exp(\alpha_k)} \]

  where \(\alpha_j\) are unconstrained reals.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="4G", venv="nwas") :dir /scratch/midway2/aksarkar/ash

  #+RESULTS:
  : Submitted batch job 43407569

  #+BEGIN_SRC ipython
    import numpy as np
    import scipy.optimize as so
    import scipy.stats as st
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[28]:
  :END:

* Implementation

  For simplicity, assume:

  1. The likelihood is Gaussian
  2. The prior is a scale mixture of Gaussians, with the default grid of scales
     as in [[https://academic.oup.com/biostatistics/article/18/2/275/2557030][Stephens 2016]].

  #+BEGIN_SRC ipython
    def softmax(logits):
      shifted = np.exp(logits - logits.max())
      return shifted / shifted.sum()

    def loss(logits, llik):
      return (llik * softmax(logits)).mean()

    def ash(beta, se, grid=None):
      if grid is None:
        start = se.min() / 10
        m = max(beta - se)
        if m < 0:
          end = 8 * start
        else:
          end = 2 * np.sqrt(m)
        grid = np.logspace(start, end, base=np.sqrt(2))
      assert len(grid.shape) == 1
      assert (grid > 0).all()
      var = se.reshape(-1, 1) + grid.reshape(1, -1)
      # Negative log-likelihood since we're going to minimize
      llik = .5 * (np.log(2 * np.pi * var) + np.square(beta).reshape(-1, 1) / var)
      logits = np.zeros(grid.shape)
      res = so.minimize(loss, logits, args=(llik,))
      if res.success:
        return softmax(res.x)
      else:
        raise ValueError(res.message)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[54]:
  :END:

  Simulate some data, fixing \(\hat{s}_j = 1\) for all \(j\).

  #+BEGIN_SRC ipython
    def simulate(n, pi0, logits=None, grid=None):
      assert np.isscalar(pi0)
      assert 0 < pi0 < 1
      if grid is None:
        grid = np.geomspace(.1, 1, 3)
      assert len(grid.shape) == 1
      assert (grid > 0).all()
      if logits is None:
        logits = np.ones(grid.shape)
      assert logits.shape == grid.shape
      x = np.random.normal(scale=np.random.choice(grid, p=softmax(logits), size=n))
      z = np.random.uniform(size=n) > pi0
      return x * z
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[78]:
  :END:

  #+BEGIN_SRC ipython
    n = 1000
    betahat = simulate(n=n, pi0=.75)
    ash(betahat, np.ones(n))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[79]:
  #+BEGIN_EXAMPLE
    array([  8.92559995e-01,   9.60621089e-02,   1.01922373e-02,
    1.06412495e-03,   1.09275573e-04,   1.10425906e-05,
    1.09675232e-06,   1.07206550e-07,   1.02870759e-08,
    9.72787089e-10,   9.02964735e-11,   8.25066670e-12,
    7.40508675e-13,   6.53751506e-14,   5.67310718e-15,
    4.84417672e-16,   4.06866102e-17,   3.35532286e-18,
    2.72467439e-19,   2.17398141e-20,   1.70495650e-21,
    1.31616463e-22,   9.98312438e-24,   7.45352313e-25,
    5.47373138e-26,   3.94828266e-27,   2.80331192e-28,
    1.95847066e-29,   1.34534376e-30,   9.08698878e-32,
    6.04150487e-33,   3.94807035e-34,   2.53958676e-35,
    1.60682208e-36,   1.00035713e-37,   6.12808610e-39,
    3.69383294e-40,   2.19163452e-41,   1.27996349e-42,
    7.35281587e-44,   4.15765392e-45,   2.31575952e-46,
    1.26826295e-47,   6.84188270e-49,   3.63312580e-50,
    1.89831331e-51,   9.77376086e-53,   4.95329407e-54,
    2.47273091e-55,   1.21506115e-56])
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :async t :results output html
    betahat = simulate(n=100000, pi0=.75)
    s = np.ones(100000)
    %timeit ash(betahat, s)
  #+END_SRC

  #+RESULTS:
  #+BEGIN_EXPORT html
  29.8 s ± 79.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
  #+END_EXPORT
